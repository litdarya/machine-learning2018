{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 14pt\">Дедлайн: 20 марта 23:59</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинное обучение, ФИВТ, Весна 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall <номер_группы> <фамилия>``, к примеру -- ``ML2018_fall 596 ivanov``\n",
    "- Выполненное дз сохраните в файл ``ML2018_<фамилия>_<группа>_task<номер задания>.ipnb``, к примеру -- ``ML2018_ivanov_596_task1.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com`` (или в телеграм-канал)\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall Question <Содержание вопроса>``\n",
    "\n",
    "--------\n",
    "- **PS1**: Используются автоматические фильтры, мы не найдем ваше дз, если вы укажете тему письма в неправильном формате.\n",
    "- **PS2**: Просроченный дедлайн снижает максимальный вес задания по формуле, указнной на первом семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Теоретические задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30% баллов за задание, оценочное время выполнения 30 минут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1 (10% баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что мы решаем задачу бинарной классификации и что у нас есть три алгоритма $b_1(x)$, $b_2(x)$ и $b_3(x)$, каждый из которых ошибается с вероятностью p. Мы строим композицию взвешенным голосованием: алгоритмам присвоены значимости $w_1$, $w_2$ и $w_3$, и для вынесения вердикта суммируются значимости алгоритмов, проголосовавших за каждый из классов:\n",
    "\n",
    "$$a_0 = \\sum_{i=1}^3 w_i [b_i(x)=0]$$\n",
    "$$a_1 = \\sum_{i=1}^3 w_i [b_i(x)=1]$$\n",
    "\n",
    "\n",
    "Объект $x$ относится к классу, для которого такая сумма оказалась максимальной. Например, если первые два алгоритма голосуют за класс $0$, а третий — за класс $1$, то выбирается класс $0$, если $w_1 + w_2 > w_3$, и класс $1$ в противном случае. Какова вероятность ошибки такой композиции этих трех алгоритмов, если:\n",
    "1. $w_1 = 0.2, w_2 = 0.3, w_3 = 0.2$;\n",
    "2. $w_1 = 0.2, w_2 = 0.5, w_3 = 0.2$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$w_1 + w_3 > w_2$$\n",
    "$$w_1 + w_2 > w_3$$\n",
    "$$w_2 + w_3 > w_1$$\n",
    "Следовательно, $$3\\cdot( p \\cdot p \\cdot (1 - p) ) + p^3 = 3\\cdot p^2 \\cdot (1 - p) + p^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $$ w_1 + w_2 > w_3$$\n",
    "\n",
    "$$w_2 + w_3 > w_1$$\n",
    "\n",
    "$$w_2 > w_1 + w_3 $$\n",
    "Следовательно,\n",
    "$$2\\cdot p^2 \\cdot (1 - p) + p^3 + p\\cdot(1 - p)^2 = p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2 (10% баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу бинарной классификации. Будем считать, что все алгоритмы из базового семейства возвращают ответы из отрезка $[0,1]$, которые можно интерпретировать как вероятности принадлежности объектов классу $1$. В качестве функции потерь возьмем отрицательный логарифм правдоподобия:\n",
    "$$L(y,z) = -(y \\log{z}+(1-y)\\log{(1-z)})$$\n",
    "В формуле $y$ - правильный ответ, $z$ - ответ алгоритма. Выпишите формулы для поиска базовых алгоритмов $b_n$ и коэффициентов $\\gamma_n$ в градиентном бустинге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3 (10% баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известно, что на $n$-й итерации двухклассового метода AdaBoost\n",
    "был выбран базовый классификатор, допускающий ошибку только на одном объекте $x_j$. Найдите нормированный вес $w_j^{(n+1)}$ при этом объекте на следующей итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как ошибается только на $x_j$:\n",
    "$$\\alpha^{(n)} = \\frac{1}{2} ln(\\frac{1 - w_j}{w_j})$$\n",
    "Подставим:\n",
    "$$w_{j}^{(n + 1)} = \\frac{w_{j}^{(n)}\\exp{(\\alpha^{(n)})}}{\\exp{(-\\alpha^{(n)})} \\cdot \\sum_{i \\neq j}w_{i}^{(n)} + \\exp{(\\alpha^{(n)})}\\cdot w_{j}^{(n)}}$$\n",
    "    \n",
    "$$ \\sum_{i \\neq j}w_{i}^{(n)} = 1 - w_{j}^{(n)}$$\n",
    "\n",
    "$$\\exp{(\\alpha^{(n)})} = \\sqrt{\\frac{1 - w_{j}^{(n)}}{w_{j}^{(n)}}}$$\n",
    "\n",
    "Поделим числитель и знаменатель дроби на числитель:\n",
    "$$w_{j}^{(n + 1)} = \\frac{1}{\\frac{1 - w_{j}^{(n)}}{1 - w_{j}^{(n)}} + 1} = \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 70% баллов за задание, оценочное время выполнения 3 часа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Необходимо реализовать класс `RandomForest`** (для решения задачи классификации)\n",
    "\n",
    "**Спецификация:**\n",
    "- класс наследуется от `sklearn.BaseEstimator`;\n",
    "- конструктор содержит следующие параметры: \n",
    "    - `num_trees` - количество деревьев в лесе;\n",
    "    - `max_depth` - максимальная глубина дерева (по умолчанию - `numpy.inf`); \n",
    "    - `max_features` - количество признаков, принимаемое к рассмотрению при разбиении (аналогичный параметр есть в sklearn имплементации). Параметр может принимать значения:\n",
    "        - int - тогда рассматриваем max_features признаков при каждом разбиении;\n",
    "        - float - max_features обозначает процент, int(max_features * n_features) признаков рассматривается при каждом разбиении;\n",
    "        - “sqrt” - max_features=sqrt(n_features);\n",
    "        - “log2” - max_features=log2(n_features);\n",
    "        - None - max_features=n_features;\n",
    "    - `criterion` - критерий разбиения (для классификации - 'gini' или 'entropy', по умолчанию - 'gini'); функции с подсчетом энтропийного и критерия Джини можно взять из предыдущего дз;\n",
    "    \n",
    "- класс имеет методы `fit` и `predict`;\n",
    "- метод `fit` принимает матрицу объектов `X` и вектор ответов `y` (объекты `numpy.ndarray`) и возвращает экземпляр класса\n",
    "    `RandomForest`, представляющий собой Random Forest, обученный по выборке `(X, y)` с учётом заданных в конструкторе параметров; \n",
    "- метод `predict` принимает матрицу объектов и возвращает вектор предсказанных ответов;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSM(X, y, size):\n",
    "    features = []\n",
    "    for i in range(size):\n",
    "        features.append(random.randint(0, X.shape[1] - 1))\n",
    "    features = list(set(features))\n",
    "    sample_X, sample_y = X[:, features], y\n",
    "    return sample_X, sample_y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(X, y, size):\n",
    "    ind = []\n",
    "    for i in range(size):\n",
    "        ind.append(random.randint(0, X.shape[0] - 1))\n",
    "    sample_X, sample_y = X[ind, :], y[ind]\n",
    "    return sample_X, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestMod(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, num_trees, max_depth=None, max_features=None, criterion='gini'):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.features = []\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def _set_max_features_(self):        \n",
    "        if isinstance(self.max_features, float):\n",
    "            self.max_features = self.max_features*self.n_features\n",
    "        elif isinstance(self.max_features, str):\n",
    "            if self.max_features == 'sqrt':\n",
    "                self.max_features = np.sqrt(self.n_features)\n",
    "            elif self.max_features == 'log2':\n",
    "                self.max_features = np.log2(self.n_features)\n",
    "        elif self.max_depth is None:\n",
    "            self.max_features = self.n_features\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.n_features = X_train.shape[1]\n",
    "        self._set_max_features_()\n",
    "        for i in range(self.num_trees):\n",
    "            X, y, features = RSM(X_train, y_train, self.max_features)\n",
    "            model = tree.DecisionTreeClassifier(criterion=self.criterion, max_depth = self.max_depth)\n",
    "            model.fit(X, y)\n",
    "            self.trees.append(model)\n",
    "            self.features.append(features)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        res = []\n",
    "        for tree, i in zip(self.trees, range(self.num_trees)):\n",
    "            res.append(tree.predict(X_test[:, self.features[i]]))\n",
    "            \n",
    "        res = np.array(res)\n",
    "        y_pred = np.zeros(X_test.shape[0], dtype=np.int)\n",
    "        \n",
    "        for col in range(X_test.shape[0]):\n",
    "            y_pred[col] = np.bincount(res[:, col]).argmax()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, num_trees, max_depth=None, max_features=None, criterion='gini'):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def _set_max_features_(self):        \n",
    "        if isinstance(self.max_features, float):\n",
    "            self.max_features = self.max_features*self.n_features\n",
    "        elif isinstance(self.max_features, str):\n",
    "            if self.max_features == 'sqrt':\n",
    "                self.max_features = np.sqrt(self.n_features)\n",
    "            elif self.max_features == 'log2':\n",
    "                self.max_features = np.log2(self.n_features)\n",
    "        elif self.max_depth is None:\n",
    "            self.max_features = self.n_features\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.n_features = X_train.shape[1]\n",
    "        self._set_max_features_()\n",
    "        for i in range(self.num_trees):\n",
    "            X, y = bagging(X_train, y_train, X_train.shape[0])\n",
    "            model = tree.DecisionTreeClassifier(criterion=self.criterion, max_depth = self.max_depth, max_features=self.max_features)\n",
    "            model.fit(X, y)\n",
    "            self.trees.append(model)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        res = []\n",
    "        for tree, i in zip(self.trees, range(self.num_trees)):\n",
    "            res.append(tree.predict(X_test))\n",
    "            \n",
    "        res = np.array(res)\n",
    "        y_pred = np.zeros(X_test.shape[0], dtype=np.int)\n",
    "        \n",
    "        for col in range(X_test.shape[0]):\n",
    "            y_pred[col] = np.bincount(res[:, col]).argmax()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите датасет Wine Data Set (https://archive.ics.uci.edu/ml/datasets/wine). Разделите выборку на обучающую и тестовую с помощью метода `train_test_split`, используйте значения параметров `test_size=0.2`, `random_state=42`. Попробуйте обучить Random Forest на предложенном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "columns = ['class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "data.columns = columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train, y_train = np.array(train[columns[1:]]), np.array(train['class'])\n",
    "X_test, y_test = np.array(test[columns[1:]]), np.array(test['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForest(criterion='gini', max_depth=None, max_features=13, num_trees=20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForest(num_trees=20, max_features=5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for k in range(100):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc.append(accuracy_score(y_pred, y_test))\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, как менялись значения критерия качества `accuracy` при увеличении параметра num_trees. Видны ли следы переобучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 10 0.9686111111111111\n",
      "n: 110 0.9699999999999998\n",
      "n: 210 0.9705555555555556\n",
      "n: 310 0.970972222222222\n",
      "n: 410 0.9712222222222221\n",
      "n: 510 0.9713888888888887\n",
      "n: 610 0.9715079365079363\n",
      "n: 710 0.971597222222222\n",
      "n: 810 0.9716666666666665\n",
      "n: 910 0.9717222222222219\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for n in range(10, 1000, 100):\n",
    "    for k in range(100):\n",
    "        model = RandomForest(num_trees=n, max_features=5)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc.append(accuracy_score(y_pred, y_test))\n",
    "    print(\"n: {}\".format(n), np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество работы вашей реализации RandomForest и реализации из sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)   \n",
    "print(accuracy_score(rfc.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модификация Random Forest (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измените свою реализацию `RandomForest` так, чтобы случайное подмножество признаков выбиралось не в каждом сплите, а перед построением всего дерева. Сравните результат работы с обычным RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestMod(num_trees=20, max_features=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "acc = []\n",
    "for k in range(100):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc.append(accuracy_score(y_pred, y_test))\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 10 0.9694444444444444\n",
      "n: 110 0.9727777777777775\n",
      "n: 210 0.9736111111111112\n",
      "n: 310 0.9735416666666664\n",
      "n: 410 0.9734999999999998\n",
      "n: 510 0.973287037037037\n",
      "n: 610 0.9731746031746031\n",
      "n: 710 0.9730555555555555\n",
      "n: 810 0.9729629629629628\n",
      "n: 910 0.9728888888888886\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for n in range(10, 1000, 100):\n",
    "    for k in range(100):\n",
    "        model = RandomForestMod(num_trees=n, max_features=5)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc.append(accuracy_score(y_pred, y_test))\n",
    "    print(\"n: {}\".format(n), np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
